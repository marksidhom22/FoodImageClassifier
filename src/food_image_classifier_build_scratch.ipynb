{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuHV5yYIxEN_"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz -O /content/food-101.tar.gz\n",
        "\n",
        "# Extract the dataset\n",
        "!tar -xzvf /content/food-101.tar.gz -C /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set data directory paths\n",
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "base_dir = '/content/food-101/images'\n",
        "classes = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
        "\n",
        "# Create train, validation, and test directories\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "for cls in classes:\n",
        "    os.makedirs(os.path.join(train_dir, cls), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, cls), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, cls), exist_ok=True)\n",
        "\n",
        "    # Get all images in class directory\n",
        "    images = os.listdir(os.path.join(base_dir, cls))\n",
        "    np.random.shuffle(images)\n",
        "\n",
        "    # Split images into train, val, and test\n",
        "    train_images = images[:int(0.7 * len(images))]\n",
        "    val_images = images[int(0.7 * len(images)):int(0.85 * len(images))]\n",
        "    test_images = images[int(0.85 * len(images)):]\n",
        "\n",
        "    # Move images to respective directories\n",
        "    for img in train_images:\n",
        "        shutil.move(os.path.join(base_dir, cls, img), os.path.join(train_dir, cls))\n",
        "\n",
        "    for img in val_images:\n",
        "        shutil.move(os.path.join(base_dir, cls, img), os.path.join(val_dir, cls))\n",
        "\n",
        "    for img in test_images:\n",
        "        shutil.move(os.path.join(base_dir, cls, img), os.path.join(test_dir, cls))\n"
      ],
      "metadata": {
        "id": "TxA8ZQFDxdQS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "c4eG_4z8xJXw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical')\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFwSzTzNxKww",
        "outputId": "ee1e26cd-71b3-42d0-c6b0-a81ef9dcd7aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 70700 images belonging to 101 classes.\n",
            "Found 15150 images belonging to 101 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flattening and Dense layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(101, activation='softmax'))  # 101 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "nab2cReYxO_c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "checkpoint_path = \"model_checkpoint_epoch_{epoch:02d}_loss_{val_loss:.2f}.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1,\n",
        "                             save_best_only=False, mode='auto', save_freq='epoch')\n"
      ],
      "metadata": {
        "id": "TLWE2RDZ5ymi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,  # depends on your data\n",
        "    epochs=20,  # adjust based on your requirement\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=50,\n",
        "    callbacks=[checkpoint]) # depends on your data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wqPgwlLxQmm",
        "outputId": "9e055804-1d8b-4f71-f3d8-9fd7bb75372e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6157 - accuracy: 0.0116\n",
            "Epoch 1: saving model to model_checkpoint_epoch_01_loss_4.62.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 56s 565ms/step - loss: 4.6157 - accuracy: 0.0116 - val_loss: 4.6158 - val_accuracy: 0.0100\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6161 - accuracy: 0.0097\n",
            "Epoch 2: saving model to model_checkpoint_epoch_02_loss_4.62.h5\n",
            "100/100 [==============================] - 63s 634ms/step - loss: 4.6161 - accuracy: 0.0097 - val_loss: 4.6164 - val_accuracy: 0.0056\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6152 - accuracy: 0.0097\n",
            "Epoch 3: saving model to model_checkpoint_epoch_03_loss_4.61.h5\n",
            "100/100 [==============================] - 65s 652ms/step - loss: 4.6152 - accuracy: 0.0097 - val_loss: 4.6125 - val_accuracy: 0.0106\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6170 - accuracy: 0.0106\n",
            "Epoch 4: saving model to model_checkpoint_epoch_04_loss_4.62.h5\n",
            "100/100 [==============================] - 58s 580ms/step - loss: 4.6170 - accuracy: 0.0106 - val_loss: 4.6159 - val_accuracy: 0.0094\n",
            "Epoch 5/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6139 - accuracy: 0.0097\n",
            "Epoch 5: saving model to model_checkpoint_epoch_05_loss_4.60.h5\n",
            "100/100 [==============================] - 64s 637ms/step - loss: 4.6139 - accuracy: 0.0097 - val_loss: 4.6040 - val_accuracy: 0.0213\n",
            "Epoch 6/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6175 - accuracy: 0.0069\n",
            "Epoch 6: saving model to model_checkpoint_epoch_06_loss_4.62.h5\n",
            "100/100 [==============================] - 67s 666ms/step - loss: 4.6175 - accuracy: 0.0069 - val_loss: 4.6169 - val_accuracy: 0.0106\n",
            "Epoch 7/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6164 - accuracy: 0.0106\n",
            "Epoch 7: saving model to model_checkpoint_epoch_07_loss_4.62.h5\n",
            "100/100 [==============================] - 61s 606ms/step - loss: 4.6164 - accuracy: 0.0106 - val_loss: 4.6154 - val_accuracy: 0.0094\n",
            "Epoch 8/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6156 - accuracy: 0.0137\n",
            "Epoch 8: saving model to model_checkpoint_epoch_08_loss_4.62.h5\n",
            "100/100 [==============================] - 60s 605ms/step - loss: 4.6156 - accuracy: 0.0137 - val_loss: 4.6181 - val_accuracy: 0.0056\n",
            "Epoch 9/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6162 - accuracy: 0.0088\n",
            "Epoch 9: saving model to model_checkpoint_epoch_09_loss_4.62.h5\n",
            "100/100 [==============================] - 64s 642ms/step - loss: 4.6162 - accuracy: 0.0088 - val_loss: 4.6156 - val_accuracy: 0.0094\n",
            "Epoch 10/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6167 - accuracy: 0.0116\n",
            "Epoch 10: saving model to model_checkpoint_epoch_10_loss_4.61.h5\n",
            "100/100 [==============================] - 57s 575ms/step - loss: 4.6167 - accuracy: 0.0116 - val_loss: 4.6150 - val_accuracy: 0.0144\n",
            "Epoch 11/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6149 - accuracy: 0.0084\n",
            "Epoch 11: saving model to model_checkpoint_epoch_11_loss_4.62.h5\n",
            "100/100 [==============================] - 60s 605ms/step - loss: 4.6149 - accuracy: 0.0084 - val_loss: 4.6152 - val_accuracy: 0.0100\n",
            "Epoch 12/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6158 - accuracy: 0.0106\n",
            "Epoch 12: saving model to model_checkpoint_epoch_12_loss_4.62.h5\n",
            "100/100 [==============================] - 66s 664ms/step - loss: 4.6158 - accuracy: 0.0106 - val_loss: 4.6157 - val_accuracy: 0.0075\n",
            "Epoch 13/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6160 - accuracy: 0.0094\n",
            "Epoch 13: saving model to model_checkpoint_epoch_13_loss_4.62.h5\n",
            "100/100 [==============================] - 61s 610ms/step - loss: 4.6160 - accuracy: 0.0094 - val_loss: 4.6153 - val_accuracy: 0.0137\n",
            "Epoch 14/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6159 - accuracy: 0.0109\n",
            "Epoch 14: saving model to model_checkpoint_epoch_14_loss_4.61.h5\n",
            "100/100 [==============================] - 65s 650ms/step - loss: 4.6159 - accuracy: 0.0109 - val_loss: 4.6145 - val_accuracy: 0.0044\n",
            "Epoch 15/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6159 - accuracy: 0.0100\n",
            "Epoch 15: saving model to model_checkpoint_epoch_15_loss_4.61.h5\n",
            "100/100 [==============================] - 57s 569ms/step - loss: 4.6159 - accuracy: 0.0100 - val_loss: 4.6148 - val_accuracy: 0.0088\n",
            "Epoch 16/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6146 - accuracy: 0.0128\n",
            "Epoch 16: saving model to model_checkpoint_epoch_16_loss_4.62.h5\n",
            "100/100 [==============================] - 61s 610ms/step - loss: 4.6146 - accuracy: 0.0128 - val_loss: 4.6157 - val_accuracy: 0.0137\n",
            "Epoch 17/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6150 - accuracy: 0.0125\n",
            "Epoch 17: saving model to model_checkpoint_epoch_17_loss_4.62.h5\n",
            "100/100 [==============================] - 59s 589ms/step - loss: 4.6150 - accuracy: 0.0125 - val_loss: 4.6159 - val_accuracy: 0.0069\n",
            "Epoch 18/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6167 - accuracy: 0.0066\n",
            "Epoch 18: saving model to model_checkpoint_epoch_18_loss_4.61.h5\n",
            "100/100 [==============================] - 58s 583ms/step - loss: 4.6167 - accuracy: 0.0066 - val_loss: 4.6132 - val_accuracy: 0.0106\n",
            "Epoch 19/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6155 - accuracy: 0.0116\n",
            "Epoch 19: saving model to model_checkpoint_epoch_19_loss_4.61.h5\n",
            "100/100 [==============================] - 57s 573ms/step - loss: 4.6155 - accuracy: 0.0116 - val_loss: 4.6146 - val_accuracy: 0.0137\n",
            "Epoch 20/20\n",
            "100/100 [==============================] - ETA: 0s - loss: 4.6155 - accuracy: 0.0084\n",
            "Epoch 20: saving model to model_checkpoint_epoch_20_loss_4.61.h5\n",
            "100/100 [==============================] - 61s 610ms/step - loss: 4.6155 - accuracy: 0.0084 - val_loss: 4.6148 - val_accuracy: 0.0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False)\n",
        "\n",
        "eval_result = model.evaluate(test_generator)\n",
        "print(f'Test Loss: {eval_result[0]}, Test Accuracy: {eval_result[1]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcfnTA2hxSVw",
        "outputId": "71092acb-4eaf-4716-da54-54c4ef24ef31"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15150 images belonging to 101 classes.\n",
            "474/474 [==============================] - 63s 133ms/step - loss: 4.6155 - accuracy: 0.0099\n",
            "Test Loss: 4.615548133850098, Test Accuracy: 0.009900989942252636\n"
          ]
        }
      ]
    }
  ]
}